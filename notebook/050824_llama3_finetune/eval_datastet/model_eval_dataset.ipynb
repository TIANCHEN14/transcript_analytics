{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "045631c3-256b-46b9-9ea8-e186c01df507",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, AutoConfig, BitsAndBytesConfig, GenerationConfig, TrainingArguments\n",
    "from peft import LoraConfig, PeftModel\n",
    "from datasets import Dataset, load_dataset\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d9a96c6-035a-4baf-96dc-f8b0535f2ca1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# this is the output string looks like we need to figure out an way to make it become an dict so we can easily process it\n",
    "# ['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are an radio transcript message transcript assisant, please classifer the following message<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nOverall balance is really good<|eot_id|><|start_header_id|>assiant<|end_header_id|>\\n\\nVehicle handling<|eot_id|>']\n",
    "# Here we load the tokenizer to do apply chat template\n",
    "\n",
    "# model name\n",
    "model_name = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "\n",
    "# tokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast = True)\n",
    "\n",
    "# add special token padding for that\n",
    "tokenizer.add_special_tokens({\"pad_token\" : \"<pad>\"})\n",
    "tokenizer.padding_side = 'right'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e9a6c16-8456-4921-b158-9783d99a35c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "976\n"
     ]
    }
   ],
   "source": [
    "# here is the part that we import the test dataset\n",
    "import json\n",
    "from tkinter import Tk\n",
    "from tkinter.filedialog import askopenfilename\n",
    "\n",
    "filename = askopenfilename()\n",
    "\n",
    "main_list = []\n",
    "\n",
    "with open (filename , \"r\") as f:\n",
    "    main_list = json.load(f)\n",
    "    \n",
    "print(len(main_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "69b4dbe8-ce3e-468a-a20d-aafdb7250b5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are an radio transcript message transcript assisant, please classifer the following message'},\n",
       " {'role': 'user', 'content': ' I got a little bit maybe fast if anything'},\n",
       " {'role': 'assiant', 'content': 'General Spotting'}]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_list[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7d632ac7-7617-42db-aa3f-c26d875f58b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# this is the part to take in the raw token and put them into \n",
    "def inverse_template(raw_input):\n",
    "        # We know the roles we are expecting\n",
    "    roles = ['system', 'user', 'assiant']\n",
    "\n",
    "    # Dictionary to hold the role and corresponding messages\n",
    "    messages = {}\n",
    "\n",
    "    # Temporary string for processing\n",
    "    temp_string = raw_input\n",
    "    \n",
    "    # create an message body\n",
    "    temp_list = []\n",
    "    \n",
    "    for role in roles:\n",
    "        # Find where each role starts\n",
    "        start_index = temp_string.lower().find(role)\n",
    "\n",
    "        # If the role is found and it's not the last role\n",
    "        if start_index != -1 and role != roles[-1]:\n",
    "            # Find where the next role starts by checking subsequent roles\n",
    "            next_role_index = float('inf')  # Start with an impossibly high index\n",
    "            for next_role in roles:\n",
    "                if next_role == role:\n",
    "                    continue\n",
    "                current_index = temp_string.lower().find(next_role)\n",
    "                if current_index != -1:\n",
    "                    next_role_index = min(next_role_index, current_index)\n",
    "\n",
    "            # Extract the message for the current role\n",
    "            if next_role_index != float('inf'):\n",
    "                messages[role] = temp_string[start_index + len(role) + 2:next_role_index].strip()\n",
    "                # Update the temp_string to the unprocessed part\n",
    "                temp_string = temp_string[next_role_index:]\n",
    "            else:\n",
    "                messages[role] = temp_string[start_index + len(role) + 2:].strip()\n",
    "        else:\n",
    "            # This is the last role\n",
    "            messages[role] = temp_string[start_index + len(role) + 2:].strip()\n",
    "\n",
    "    # Display the extracted messages\n",
    "    for role, message in messages.items():\n",
    "        new_message = message.replace(\"end_header_id|>\\n\\n\" , \"\").strip()\n",
    "        new_message = new_message.replace(\"<|eot_id|>\" , \"\").strip()\n",
    "        new_message = new_message.replace(\"<|start_header_id|>\" , \"\").strip()\n",
    "        temp_dict = {}\n",
    "        temp_dict['role'] = role\n",
    "        temp_dict['content'] = new_message\n",
    "        temp_list.append(temp_dict)\n",
    "    \n",
    "    return temp_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "51a44dc4-ad5c-43ce-a0c6-7fcecf30f3f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an radio transcript message transcript assisant, please classifer the following message<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Alright, we'll go single file now.<|eot_id|><|start_header_id|>assiant<|end_header_id|>\n",
      "\n",
      "General Spotting<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "message = tokenizer.apply_chat_template(main_list[0],\n",
    "                                        tokenize = False\n",
    "                                       ) \n",
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "33d97b2e-54ce-4136-8f2d-51023fa700c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are an radio transcript message transcript assisant, please classifer the following message'}, {'role': 'user', 'content': \"Alright, we'll go single file now.\"}, {'role': 'assiant', 'content': 'General Spotting'}]\n"
     ]
    }
   ],
   "source": [
    "new_dict = inverse_template(message)\n",
    "print(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eadaf2-a67f-412e-aeb0-bb5e5e2c2040",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_message = tokenizer.batch_decode(message,\n",
    "                                       skip_special_token = False,\n",
    "                                       clean_up_tokenization_spaces = 1)\n",
    "\n",
    "print(output_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3251d93-4eb1-40a2-a14e-c0a5066105f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define compute module\n",
    "compute_dtype = getattr(torch, 'float16')\n",
    "\n",
    "# Quantization parameter\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_compute_dtype = compute_dtype,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    \n",
    ")\n",
    "\n",
    "# load in the base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    #quantization_config = bnb_config,\n",
    "    device_map = \"auto\"\n",
    ")\n",
    "\n",
    "# uniform the input text length\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "# Configure the pad token in the model\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "tokenizer.padding_side = 'right'\n",
    "model.config.use_cache = False\n",
    "\n",
    "model = PeftModel.from_pretrained(model, \"./results/run_1/checkpoint-200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0338507c-1b6b-418f-aeb7-84c20b1286cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are an radio transcript message transcript assisant, please classifer the following message'},\n",
       " {'role': 'user', 'content': \" Alright, we'll go single file now.\"},\n",
       " {'role': 'target assiant', 'content': 'General Spotting'}]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loop though all the main list and rename the assiant\n",
    "\n",
    "for index, message in enumerate(main_list):\n",
    "    \n",
    "    for message_index, content in enumerate(message):\n",
    "        if content['role'] == \"assiant\":\n",
    "            main_list[index][message_index]['role'] = \"target assiant\"\n",
    "            \n",
    "    # pop the target assistant out\n",
    "    message.pop()\n",
    "    \n",
    "    # pass it though the tokenizer\n",
    "    prompt = tokenizer.apply_chat_template(message,\n",
    "                                                  tokenize = True,\n",
    "                                                  return_tensor = \"pt\"\n",
    "    )\n",
    "    \n",
    "    # push input to GPU\n",
    "    model_input = prompt.cuda()\n",
    "    \n",
    "    # generate output\n",
    "    generated_output = model.generate(\n",
    "                                        model_input,\n",
    "                                        max_new_tokens = 20,\n",
    "                                        do_sample = True\n",
    "                                        )\n",
    "    \n",
    "    # break the output into dict object\n",
    "    output_dict = inverser_template(generated_output)\n",
    "    \n",
    "    # append result into the main list\n",
    "    for output_index, content in enumerate(output_dict):\n",
    "        if content['role'] == \"assiant\":\n",
    "            main_list[index][output_index].append(content)    \n",
    "    \n",
    "    \n",
    "    \n",
    "            \n",
    "main_list[10]\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183453f5-d640-42c4-b3a9-c04cb1bc599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
