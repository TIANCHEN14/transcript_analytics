{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "045631c3-256b-46b9-9ea8-e186c01df507",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, AutoConfig, BitsAndBytesConfig, GenerationConfig, TrainingArguments\n",
    "from peft import LoraConfig, PeftModel\n",
    "from datasets import Dataset, load_dataset\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d9a96c6-035a-4baf-96dc-f8b0535f2ca1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# this is the output string looks like we need to figure out an way to make it become an dict so we can easily process it\n",
    "# ['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are an radio transcript message transcript assisant, please classifer the following message<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nOverall balance is really good<|eot_id|><|start_header_id|>assiant<|end_header_id|>\\n\\nVehicle handling<|eot_id|>']\n",
    "# Here we load the tokenizer to do apply chat template\n",
    "\n",
    "# model name\n",
    "model_name = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "\n",
    "# tokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast = True)\n",
    "\n",
    "# add special token padding for that\n",
    "tokenizer.add_special_tokens({\"pad_token\" : \"<pad>\"})\n",
    "tokenizer.padding_side = 'right'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e9a6c16-8456-4921-b158-9783d99a35c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "976\n"
     ]
    }
   ],
   "source": [
    "# here is the part that we import the test dataset\n",
    "import json\n",
    "from tkinter import Tk\n",
    "from tkinter.filedialog import askopenfilename\n",
    "\n",
    "filename = askopenfilename()\n",
    "\n",
    "main_list = []\n",
    "\n",
    "with open (filename , \"r\") as f:\n",
    "    main_list = json.load(f)\n",
    "    \n",
    "print(len(main_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "69b4dbe8-ce3e-468a-a20d-aafdb7250b5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are an radio transcript message transcript assisant, please classifer the following message'},\n",
       " {'role': 'user', 'content': ' I got a little bit maybe fast if anything'},\n",
       " {'role': 'assiant', 'content': 'General Spotting'}]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_list[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7d632ac7-7617-42db-aa3f-c26d875f58b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# this is the part to take in the raw token and put them into \n",
    "def inverse_template(raw_input):\n",
    "        # We know the roles we are expecting\n",
    "    roles = ['system', 'user', 'assiant']\n",
    "\n",
    "    # Dictionary to hold the role and corresponding messages\n",
    "    messages = {}\n",
    "\n",
    "    # Temporary string for processing\n",
    "    temp_string = raw_input\n",
    "    \n",
    "    # create an message body\n",
    "    temp_list = []\n",
    "    \n",
    "    for role in roles:\n",
    "        # Find where each role starts\n",
    "        start_index = temp_string.lower().find(role)\n",
    "\n",
    "        # If the role is found and it's not the last role\n",
    "        if start_index != -1 and role != roles[-1]:\n",
    "            # Find where the next role starts by checking subsequent roles\n",
    "            next_role_index = float('inf')  # Start with an impossibly high index\n",
    "            for next_role in roles:\n",
    "                if next_role == role:\n",
    "                    continue\n",
    "                current_index = temp_string.lower().find(next_role)\n",
    "                if current_index != -1:\n",
    "                    next_role_index = min(next_role_index, current_index)\n",
    "\n",
    "            # Extract the message for the current role\n",
    "            if next_role_index != float('inf'):\n",
    "                messages[role] = temp_string[start_index + len(role) + 2:next_role_index].strip()\n",
    "                # Update the temp_string to the unprocessed part\n",
    "                temp_string = temp_string[next_role_index:]\n",
    "            else:\n",
    "                messages[role] = temp_string[start_index + len(role) + 2:].strip()\n",
    "        else:\n",
    "            # This is the last role\n",
    "            messages[role] = temp_string[start_index + len(role) + 2:].strip()\n",
    "\n",
    "    # Display the extracted messages\n",
    "    for role, message in messages.items():\n",
    "        new_message = message.replace(\"end_header_id|>\\n\\n\" , \"\").strip()\n",
    "        new_message = new_message.replace(\"<|eot_id|>\" , \"\").strip()\n",
    "        new_message = new_message.replace(\"<|start_header_id|>\" , \"\").strip()\n",
    "        temp_dict = {}\n",
    "        temp_dict['role'] = role\n",
    "        temp_dict['content'] = new_message\n",
    "        temp_list.append(temp_dict)\n",
    "    \n",
    "    return temp_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "51a44dc4-ad5c-43ce-a0c6-7fcecf30f3f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an radio transcript message transcript assisant, please classifer the following message<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Alright, we'll go single file now.<|eot_id|><|start_header_id|>assiant<|end_header_id|>\n",
      "\n",
      "General Spotting<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "message = tokenizer.apply_chat_template(main_list[0],\n",
    "                                        tokenize = False\n",
    "                                       ) \n",
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "33d97b2e-54ce-4136-8f2d-51023fa700c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are an radio transcript message transcript assisant, please classifer the following message'}, {'role': 'user', 'content': \"Alright, we'll go single file now.\"}, {'role': 'assiant', 'content': 'General Spotting'}]\n"
     ]
    }
   ],
   "source": [
    "new_dict = inverse_template(message)\n",
    "print(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eadaf2-a67f-412e-aeb0-bb5e5e2c2040",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_message = tokenizer.batch_decode(message,\n",
    "                                       skip_special_token = False,\n",
    "                                       clean_up_tokenization_spaces = 1)\n",
    "\n",
    "print(output_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e3251d93-4eb1-40a2-a14e-c0a5066105f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebaaa677c48c4200ab7080221e853566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 14\u001b[0m\n\u001b[0;32m      5\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[0;32m      6\u001b[0m     load_in_4bit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      7\u001b[0m     bnb_4bit_quant_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# load in the base model\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#quantization_config = bnb_config,\u001b[39;49;00m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     18\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# uniform the input text length\u001b[39;00m\n\u001b[0;32m     21\u001b[0m model\u001b[38;5;241m.\u001b[39mresize_token_embeddings(\u001b[38;5;28mlen\u001b[39m(tokenizer))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:3531\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3523\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[0;32m   3524\u001b[0m     (\n\u001b[0;32m   3525\u001b[0m         model,\n\u001b[0;32m   3526\u001b[0m         missing_keys,\n\u001b[0;32m   3527\u001b[0m         unexpected_keys,\n\u001b[0;32m   3528\u001b[0m         mismatched_keys,\n\u001b[0;32m   3529\u001b[0m         offload_index,\n\u001b[0;32m   3530\u001b[0m         error_msgs,\n\u001b[1;32m-> 3531\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3534\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[0;32m   3535\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3536\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3537\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3538\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3539\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3542\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3543\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3547\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3549\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[0;32m   3550\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:3958\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[1;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules)\u001b[0m\n\u001b[0;32m   3954\u001b[0m                 set_module_tensor_to_device(\n\u001b[0;32m   3955\u001b[0m                     model_to_load, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m*\u001b[39mparam\u001b[38;5;241m.\u001b[39msize(), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   3956\u001b[0m                 )\n\u001b[0;32m   3957\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3958\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3959\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3960\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3961\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloaded_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3962\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3963\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3964\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3965\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3966\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3967\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3968\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3969\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3970\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3971\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3972\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3973\u001b[0m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3974\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3975\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[0;32m   3976\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:778\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[1;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys)\u001b[0m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m old_param \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 778\u001b[0m         param \u001b[38;5;241m=\u001b[39m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mold_param\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m old_param\u001b[38;5;241m.\u001b[39mis_contiguous():\n\u001b[0;32m    781\u001b[0m         param \u001b[38;5;241m=\u001b[39m param\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define compute module\n",
    "compute_dtype = getattr(torch, 'float16')\n",
    "\n",
    "# Quantization parameter\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_compute_dtype = compute_dtype,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    \n",
    ")\n",
    "\n",
    "# load in the base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    #quantization_config = bnb_config,\n",
    "    device_map = \"auto\"\n",
    ")\n",
    "\n",
    "# uniform the input text length\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "# Configure the pad token in the model\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "tokenizer.padding_side = 'right'\n",
    "model.config.use_cache = False\n",
    "\n",
    "model = PeftModel.from_pretrained(model, \"./results/run_1/checkpoint-200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0338507c-1b6b-418f-aeb7-84c20b1286cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are an radio transcript message transcript assisant, please classifer the following message'}]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m model_input \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# generate output\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m generated_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m     25\u001b[0m                                     model_input,\n\u001b[0;32m     26\u001b[0m                                     max_new_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m     27\u001b[0m                                     do_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     28\u001b[0m                                     )\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# break the output into dict object\u001b[39;00m\n\u001b[0;32m     31\u001b[0m output_dict \u001b[38;5;241m=\u001b[39m inverser_template(generated_output)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# loop though all the main list and rename the assiant\n",
    "\n",
    "for index, message in enumerate(main_list):\n",
    "    \n",
    "    for message_index, content in enumerate(message):\n",
    "        if content['role'] == \"assiant\":\n",
    "            main_list[index][message_index]['role'] = \"target assiant\"\n",
    "            \n",
    "    # pop the target assistant out\n",
    "    message.pop()\n",
    "    \n",
    "    print(message)\n",
    "    \n",
    "    # pass it though the tokenizer\n",
    "    prompt = tokenizer.apply_chat_template(message,\n",
    "                                            tokenize = True,\n",
    "                                            return_tensors = \"pt\"\n",
    "    )\n",
    "    \n",
    "    # push input to GPU\n",
    "    model_input = prompt.cuda()\n",
    "    \n",
    "    # generate output\n",
    "    generated_output = model.generate(\n",
    "                                        model_input,\n",
    "                                        max_new_tokens = 20,\n",
    "                                        do_sample = True\n",
    "                                        )\n",
    "    \n",
    "    # break the output into dict object\n",
    "    output_dict = inverser_template(generated_output)\n",
    "    \n",
    "    # append result into the main list\n",
    "    for output_index, content in enumerate(output_dict):\n",
    "        if content['role'] == \"assiant\":\n",
    "            main_list[index][output_index].append(content)    \n",
    "    \n",
    "    \n",
    "    \n",
    "            \n",
    "main_list[10]\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "183453f5-d640-42c4-b3a9-c04cb1bc599e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are an radio transcript message transcript assisant, please classifer the following message'}, {'role': 'user', 'content': \" Alright, we'll go single file now.\"}]\n"
     ]
    }
   ],
   "source": [
    "new_list = main_list[0].copy()\n",
    "new_list.pop()\n",
    "print(new_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c822b11-8705-43cb-ad5e-5a790cf32d15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
