{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e7c0c56-dd08-489c-b529-7f79d8c96f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, AutoConfig, BitsAndBytesConfig, GenerationConfig, TrainingArguments\n",
    "from peft import LoraConfig, PeftModel\n",
    "from datasets import Dataset, load_dataset\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06e8f9a6-ab94-4d8d-b45b-97f631172020",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# model name\n",
    "model_name = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "\n",
    "# tokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast = True)\n",
    "\n",
    "# add special token padding for that\n",
    "tokenizer.add_special_tokens({\"pad_token\" : \"<pad>\"})\n",
    "tokenizer.padding_side = 'right'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd06d18b-1313-414d-8cde-36c82e344923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8006c01d862a49c2b166e38bca7185e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define compute module\n",
    "compute_dtype = getattr(torch, 'float16')\n",
    "\n",
    "# Quantization parameter\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_compute_dtype = compute_dtype,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    \n",
    ")\n",
    "\n",
    "# load in the base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    #quantization_config = bnb_config,\n",
    "    device_map = \"auto\"\n",
    ")\n",
    "\n",
    "# uniform the input text length\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "# Configure the pad token in the model\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "tokenizer.padding_side = 'right'\n",
    "model.config.use_cache = False\n",
    "\n",
    "model = PeftModel.from_pretrained(model, \"./results/run_1/checkpoint-200\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc5a79c0-dc5f-46be-9f97-319b9cfb88d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are an radio transcript message transcript assisant, please classifer the following message<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nOverall balance is really good<|eot_id|><|start_header_id|>assiant<|end_header_id|>\\n\\nVehicle handling<|eot_id|>']\n"
     ]
    }
   ],
   "source": [
    "messages = [{'role': 'system', 'content': 'You are an radio transcript message transcript assisant, please classifer the following message'},\n",
    "         {'role': 'user', 'content': 'Overall balance is really good'}]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    return_tensors = \"pt\"\n",
    ")\n",
    "\n",
    "#text_input = tokenizer(prompt , return_tensors = \"pt\")\n",
    "\n",
    "#print(text_input)\n",
    "\n",
    "model.to(\"cuda\")\n",
    "model_input = prompt.cuda()\n",
    "\n",
    "generation_output = model.generate(\n",
    "    model_input,\n",
    "    max_new_tokens = 20,\n",
    "    do_sample = True\n",
    ")\n",
    "\n",
    "decoded = tokenizer.batch_decode(generation_output)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ce84e9b-ec3a-495e-8cc6-b092748a249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15b25d4f-900c-4c5b-9c39-d4cb9515dd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3671d0f7-17aa-4807-965b-5662fa9ed028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./llama3_lora_run1/tokenizer_config.json',\n",
       " './llama3_lora_run1/special_tokens_map.json',\n",
       " './llama3_lora_run1/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('./llama3_lora_run1')\n",
    "tokenizer.save_pretrained('./llama3_lora_run1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388ca439-13e6-4ab9-b9d5-e4d6a132928f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf17914-fd0e-4253-8b18-d1091fd7481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub('llama3_lora_run1')\n",
    "tokenizer.push_to_hub('llama3_lora_run1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
